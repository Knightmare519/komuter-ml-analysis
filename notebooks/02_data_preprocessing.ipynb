{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b970529",
   "metadata": {},
   "source": [
    "# KTM Komuter Data Preprocessing\n",
    "\n",
    "This notebook performs preprocessing steps on the KTM Komuter dataset to prepare it for our multi-objective prediction system: \"Dynamic Route Scheduling & Anomaly Detection System\".\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Load and validate the raw data from our exploration notebook\n",
    "2. Handle any missing values or inconsistencies\n",
    "3. Handle outliers appropriately (flag them for anomaly detection later)\n",
    "4. Encode categorical variables\n",
    "5. Create a processed dataset ready for feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ff4b9",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries and load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07752778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d54e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (670599, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>ridership</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>Abdullah Hukum</td>\n",
       "      <td>Klang</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>Abdullah Hukum</td>\n",
       "      <td>Telok Pulai</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>Bangi</td>\n",
       "      <td>Batu Caves</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>Bank Negara</td>\n",
       "      <td>Sungai Gadut</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>Batu Tiga</td>\n",
       "      <td>Kampung Raja Uda</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   time          origin       destination  ridership\n",
       "0  2025-01-01  00:00  Abdullah Hukum             Klang          1\n",
       "1  2025-01-01  00:00  Abdullah Hukum       Telok Pulai          1\n",
       "2  2025-01-01  00:00           Bangi        Batu Caves          1\n",
       "3  2025-01-01  00:00     Bank Negara      Sungai Gadut          1\n",
       "4  2025-01-01  00:00       Batu Tiga  Kampung Raja Uda          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = '../data/raw/komuter_2025.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check the first few rows of the data\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d7b6e",
   "metadata": {},
   "source": [
    "## Data Type Conversion\n",
    "\n",
    "Let's convert date and time columns to proper datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8438e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date and time columns to datetime\n",
    "df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Extract useful time components\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['day'] = df['datetime'].dt.day\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['day_name'] = df['datetime'].dt.day_name()\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "# Create a route column (origin-destination pair)\n",
    "df['route'] = df['origin'] + ' â†’ ' + df['destination']\n",
    "\n",
    "# Display the updated dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980aab63",
   "metadata": {},
   "source": [
    "## Check for Missing Values and Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd322cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c031d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for 'Unknown' values in origin and destination columns\n",
    "print(\"Occurrences of 'Unknown' in origin:\", (df['origin'] == 'Unknown').sum())\n",
    "print(\"Occurrences of 'Unknown' in destination:\", (df['destination'] == 'Unknown').sum())\n",
    "\n",
    "# Check for unusual or invalid values in ridership\n",
    "print(f\"Minimum ridership value: {df['ridership'].min()}\")\n",
    "print(f\"Maximum ridership value: {df['ridership'].max()}\")\n",
    "print(f\"Number of zero ridership entries: {(df['ridership'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a79dbd3",
   "metadata": {},
   "source": [
    "## Handling Unknown Values\n",
    "\n",
    "Let's decide how to handle the 'Unknown' values in our dataset. For our prediction system, keeping them might actually be useful since they represent real patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0451a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze 'Unknown' entries to see if they follow specific patterns\n",
    "unknown_origins = df[df['origin'] == 'Unknown'].copy()\n",
    "unknown_destinations = df[df['destination'] == 'Unknown'].copy()\n",
    "\n",
    "print(f\"Number of 'Unknown' origins: {len(unknown_origins)}\")\n",
    "print(f\"Number of 'Unknown' destinations: {len(unknown_destinations)}\")\n",
    "\n",
    "# Check distribution of unknown origins by hour\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.countplot(x='hour', data=unknown_origins)\n",
    "plt.title('Distribution of Unknown Origins by Hour')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For our prediction system, we'll keep 'Unknown' values since they represent a real pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ec683",
   "metadata": {},
   "source": [
    "## Outlier Detection and Handling\n",
    "\n",
    "Since we're building a system that includes anomaly detection, we'll identify outliers but preserve them in a separate flag column rather than removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistical boundaries for outliers using IQR method\n",
    "Q1 = df['ridership'].quantile(0.25)\n",
    "Q3 = df['ridership'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Lower bound for ridership: {lower_bound}\")\n",
    "print(f\"Upper bound for ridership: {upper_bound}\")\n",
    "\n",
    "# Create a column to flag potential outliers (for use in anomaly detection later)\n",
    "df['is_statistical_outlier'] = ((df['ridership'] < lower_bound) | (df['ridership'] > upper_bound))\n",
    "\n",
    "# Calculate how many outliers were detected\n",
    "outlier_count = df['is_statistical_outlier'].sum()\n",
    "print(f\"Number of statistical outliers detected: {outlier_count} ({outlier_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Display some outlier examples\n",
    "print(\"\\nExamples of outlier records:\")\n",
    "df[df['is_statistical_outlier']].sort_values('ridership', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ffc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for route-specific outliers\n",
    "# Define a function to detect route-specific outliers\n",
    "def detect_route_outliers(group):\n",
    "    route_mean = group['ridership'].mean()\n",
    "    route_std = group['ridership'].std()\n",
    "    # Flag as outlier if more than 3 standard deviations from the route mean\n",
    "    if route_std > 0:  # Protect against routes with 0 std dev\n",
    "        group['is_route_outlier'] = abs(group['ridership'] - route_mean) > (3 * route_std)\n",
    "    else:\n",
    "        group['is_route_outlier'] = False\n",
    "    return group\n",
    "\n",
    "# Apply the function by route\n",
    "df = df.groupby('route').apply(detect_route_outliers)\n",
    "\n",
    "# Count route-specific outliers\n",
    "route_outlier_count = df['is_route_outlier'].sum()\n",
    "print(f\"Number of route-specific outliers: {route_outlier_count} ({route_outlier_count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224233f",
   "metadata": {},
   "source": [
    "## Feature Transformation\n",
    "\n",
    "Let's create additional transformed features that will be useful for our prediction system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ee9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary features for time segments and days\n",
    "df['is_weekend'] = df['day_of_week'] >= 5  # 5=Saturday, 6=Sunday\n",
    "df['is_peak_morning'] = ((df['hour'] >= 6) & (df['hour'] <= 9) & ~df['is_weekend'])\n",
    "df['is_peak_evening'] = ((df['hour'] >= 17) & (df['hour'] <= 20) & ~df['is_weekend'])\n",
    "df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 17) & ~df['is_weekend'])\n",
    "df['is_night_hours'] = ((df['hour'] >= 21) | (df['hour'] <= 5))\n",
    "\n",
    "# Display the new features\n",
    "df[['date', 'time', 'day_name', 'hour', 'is_weekend', 'is_peak_morning', 'is_peak_evening', 'is_business_hours', 'is_night_hours']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249dce89",
   "metadata": {},
   "source": [
    "## Categorical Variable Encoding\n",
    "\n",
    "For stations and routes, we'll create frequency-based encodings to capture their relative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab24cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency encodings for origin and destination stations\n",
    "origin_freq = df.groupby('origin')['ridership'].count() / len(df)\n",
    "dest_freq = df.groupby('destination')['ridership'].count() / len(df)\n",
    "\n",
    "# Create dictionaries for mapping\n",
    "origin_freq_map = origin_freq.to_dict()\n",
    "dest_freq_map = dest_freq.to_dict()\n",
    "\n",
    "# Apply the mappings\n",
    "df['origin_freq'] = df['origin'].map(origin_freq_map)\n",
    "df['destination_freq'] = df['destination'].map(dest_freq_map)\n",
    "\n",
    "# Create popularity features based on total ridership\n",
    "origin_pop = df.groupby('origin')['ridership'].sum()\n",
    "dest_pop = df.groupby('destination')['ridership'].sum()\n",
    "\n",
    "# Normalize to 0-1 range\n",
    "origin_pop = (origin_pop - origin_pop.min()) / (origin_pop.max() - origin_pop.min())\n",
    "dest_pop = (dest_pop - dest_pop.min()) / (dest_pop.max() - dest_pop.min())\n",
    "\n",
    "# Create dictionaries for mapping\n",
    "origin_pop_map = origin_pop.to_dict()\n",
    "dest_pop_map = dest_pop.to_dict()\n",
    "\n",
    "# Apply the mappings\n",
    "df['origin_popularity'] = df['origin'].map(origin_pop_map)\n",
    "df['destination_popularity'] = df['destination'].map(dest_pop_map)\n",
    "\n",
    "# Display the new features\n",
    "df[['origin', 'destination', 'origin_freq', 'destination_freq', 'origin_popularity', 'destination_popularity']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cyclical encoding for hour and day of week to capture their cyclical nature\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week']/7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week']/7)\n",
    "\n",
    "# Display the cyclical encodings\n",
    "df[['hour', 'day_of_week', 'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988995d9",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Route-Specific Analysis\n",
    "\n",
    "Let's aggregate the data by route and hour to create a dataset that's better suited for our dynamic route scheduling system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b283e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an aggregated dataset by route and hour\n",
    "route_hour_df = df.groupby(['route', 'date', 'hour']).agg(\n",
    "    total_ridership=('ridership', 'sum'),\n",
    "    avg_ridership=('ridership', 'mean'),\n",
    "    max_ridership=('ridership', 'max'),\n",
    "    day_of_week=('day_of_week', 'first'),\n",
    "    is_weekend=('is_weekend', 'first'),\n",
    "    origin=('origin', 'first'),\n",
    "    destination=('destination', 'first'),\n",
    "    origin_popularity=('origin_popularity', 'first'),\n",
    "    destination_popularity=('destination_popularity', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Add the cyclical features to the aggregated dataset\n",
    "route_hour_df['hour_sin'] = np.sin(2 * np.pi * route_hour_df['hour']/24)\n",
    "route_hour_df['hour_cos'] = np.cos(2 * np.pi * route_hour_df['hour']/24)\n",
    "route_hour_df['day_of_week_sin'] = np.sin(2 * np.pi * route_hour_df['day_of_week']/7)\n",
    "route_hour_df['day_of_week_cos'] = np.cos(2 * np.pi * route_hour_df['day_of_week']/7)\n",
    "\n",
    "# Display the aggregated dataset\n",
    "print(f\"Aggregated dataset shape: {route_hour_df.shape}\")\n",
    "route_hour_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48590d7",
   "metadata": {},
   "source": [
    "## Create Time Series Features\n",
    "\n",
    "For time series forecasting, we'll create lagged features and rolling statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
